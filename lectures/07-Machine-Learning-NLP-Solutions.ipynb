{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python as a Calculator\n",
    "\n",
    "Blank notebook to be used for class exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write code to load the data in the \"iris.csv\" into numpy arrays.\n",
    "\n",
    "The frst 4 columns are the features/attributes. The last column is the\n",
    "class. Simply load the class as a list of strings. Don't forget to convert the\n",
    "dataset into a numpy array. You can use either DictVectorizer or the CVS\n",
    "method on the previous slide to load the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\n",
      "5.4,3.9,1.7,0.4,Iris-setosa\n",
      "4.6,3.4,1.4,0.3,Iris-setosa\n",
      "5.0,3.4,1.5,0.2,Iris-setosa\n",
      "4.4,2.9,1.4,0.2,Iris-setosa\n",
      "4.9,3.1,1.5,0.1,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "with open('../data/datasets/iris/iris.csv') as in_file:\n",
    "    count = 0\n",
    "    for row in in_file:\n",
    "        print(row.strip())\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "with open('../data/datasets/iris/iris.csv') as in_file:\n",
    "    count = 0\n",
    "    for row in in_file:\n",
    "        data = row.strip().split(',')\n",
    "        feats = [float(x) for x in data[:-1]]\n",
    "        X.append(feats)\n",
    "        y.append(data[-1])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X_dicts = []\n",
    "y = []\n",
    "\n",
    "with open('../data/datasets/iris/iris.csv') as in_file:\n",
    "    count = 0\n",
    "    for row in in_file:\n",
    "        data = row.strip().split(',')\n",
    "        item = {}\n",
    "        item['feat 1'] = float(data[0])\n",
    "        item['feat 2'] = float(data[1])\n",
    "        item['feat 3'] = float(data[2])\n",
    "        item['feat 4'] = float(data[3])\n",
    "        X_dicts.append(item)\n",
    "        y.append(data[-1])\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "\n",
    "X = vec.fit_transform(X_dicts)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Using the iris data you loaded in Exercise 1, do the following:\n",
    "\n",
    "- Use train_test_split() to split the iris dataset. (use 0.2 for the\n",
    "test size)\n",
    "- Train an SVM on the train split and evaluate using accuracy on the\n",
    "test split.\n",
    "- Fiddle with the parameters of the SVM to see how it effects the\n",
    "performance.\n",
    "- Calculate the accuracy on the train split. Is there a difference between the train/test accuracies?\n",
    "\n",
    "Next, try using a different classifier, a random forest, and see how it\n",
    "compares to the SVM\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Note that this is a toy dataset, so all scores will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.3000\n",
      "Train Score: 0.3417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.0001)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Test Score: {:.4f}\".format(accuracy_score(y_test, y_test_preds)))\n",
    "print(\"Train Score: {:.4f}\".format(accuracy_score(y_train, clf.predict(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Using the train/test iris dataset split from exercise 2. Compare all combinations of parameters by looping over the SVC kernel parameters \"rbf\" and \"linear\",\n",
    "and C parameters 0.001, 0.01, 0.1, 1., and 10. Print the training and\n",
    "validation scores for every pair of parameters.\n",
    "\n",
    "\n",
    "Hint: You need to nest two for loops. You can use the train/test splits from Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.3000 Train Score: 0.3417\n",
      "Test Score: 0.3000 Train Score: 0.3417\n",
      "Test Score: 1.0000 Train Score: 0.9417\n",
      "Test Score: 1.0000 Train Score: 0.9917\n",
      "Test Score: 1.0000 Train Score: 0.9833\n",
      "Test Score: 0.3000 Train Score: 0.3417\n",
      "Test Score: 0.9667 Train Score: 0.9250\n",
      "Test Score: 1.0000 Train Score: 0.9667\n",
      "Test Score: 1.0000 Train Score: 0.9750\n",
      "Test Score: 0.9667 Train Score: 0.9667\n"
     ]
    }
   ],
   "source": [
    "for k in ['rbf', 'linear']:\n",
    "    for c in [0.001, 0.01, 0.1, 1., 10.]:\n",
    "        clf = SVC(kernel=k, C=c)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_test_preds = clf.predict(X_test)\n",
    "        y_train_preds = clf.predict(X_train)\n",
    "        print(\"Test Score: {:.4f} Train Score: {:.4f}\".format(\n",
    "            accuracy_score(y_test, y_test_preds), accuracy_score(y_train, y_train_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Use the iris dataset to create a 3-way split (train/val/test), optimize the SVC parameters using the validation split, then report the final f1 score on the test, train, and validation datasets.\n",
    "\n",
    "You will need to use the train\\_test\\_split() method on the train dataset from Exercise 2. You can use a 10% test size.\n",
    "\n",
    "How close are the validation and test scores? How does the training score compare to the test and validation scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1 kern: rbf acc: 0.9166666666666666\n",
      "C: 0.1 kern: linear acc: 0.9166666666666666\n",
      "C: 1.0 kern: rbf acc: 0.8333333333333334\n",
      "C: 1.0 kern: linear acc: 0.8333333333333334\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "for c in [0.1, 1.]:\n",
    "    for k in ['rbf', 'linear']:\n",
    "        clf = SVC(kernel = k, C=c)\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        val_preds = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_val, val_preds)\n",
    "        print(\"C: {} kern: {} acc: {}\".format(c, k, acc))\n",
    "fclf = SVC(kernel='rbf', C=0.1)\n",
    "fclf.fit(X_train2, y_train2)\n",
    "test_preds = fclf.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(test_acc)\n",
    "\n",
    "params = {\"C\": [0.1, 1.], 'kernel': ['rbf', 'linear']}\n",
    "\n",
    "gclf = GridSearchCV(SVC(), params, cv=3)\n",
    "\n",
    "gclf.fit(X_train, y_train)\n",
    "\n",
    "new_preds = gclf.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, new_preds)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES for F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "MICRO\n",
    "\n",
    "TP_i = true positives\n",
    "\n",
    "FP_i = false positives\n",
    "\n",
    "FN_i = false negatives\n",
    "\n",
    "TP = sum of all TP_i\n",
    "FP = sum of all FP_i\n",
    "FN = sum of all FN_i\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP /(TP + FN)\n",
    "\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "\n",
    "MACRO F1\n",
    "\n",
    "TP_i = true positives\n",
    "\n",
    "FP_i = false positives\n",
    "\n",
    "FN_i = false negatives\n",
    "\n",
    "\n",
    "\n",
    "precision_i = TP_i / (TP_i + FP_i)\n",
    "recall = TP_i /(TP_i + FN_i)\n",
    "\n",
    "F1_i = (2 * precision_i * recall_i) / (precision_i + recall_i)\n",
    "\n",
    "$\\frac{1}{C}\\sum_{i=1}^{i=C} F_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Project\n",
    "\n",
    "For this exercise, you will code your own k nearest neighbor method. For this assume \"k\" is equal to 1. You can use euclidean distance to find similar examples.\n",
    "\n",
    "Euclidean distance is defined as\n",
    "\n",
    "$ EDist = \\sqrt{(x_0 - v_0)^2 + (x_1 - v_1)^2 + \\dots + (x_{D-1} - v_{D-1})^2} $\n",
    "\n",
    "where D is the dimension size (number of elements) for the vectors.\n",
    "\n",
    "**Hint:** The **easiest** way to complete this exercise is with for loops. The **fastest** way to complete this exercise is to complete this exericse is to use numpy cleverness. Both approaches are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edist(x1, x2):\n",
    "    return np.linalg.norm(x1-x2)\n",
    "\n",
    "\n",
    "def kNN(query, X, y):\n",
    "    '''\n",
    "        Complete this function to return the class of the closest\n",
    "        example (row) in X to the vector \"query\" based on euclidean\n",
    "        distance.\n",
    "        :param vector query: A numpy vector (Will be one row from the test set from Exercise 2)\n",
    "        :param matrix X: A numpy matrix (will be training dataset from Exercise 2)\n",
    "        :return: Return the class (element of y) corresponding the the closes item in X to query.\n",
    "    '''\n",
    "    min_dist = None\n",
    "    pred = None\n",
    "    for xi,yi in zip(X,y):\n",
    "        if min_dist is None:\n",
    "            min_dist = edist(query, xi)\n",
    "            pred = yi\n",
    "        else:\n",
    "            dist = edist(query, xi)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                pred = yi\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for test_x1 in X_test:\n",
    "    new_pred = kNN(test_x1, X_train, y_train)\n",
    "    preds.append(new_pred)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "The tab (\\t) separated file \"sentiment-twitter-data.tsv\" contains tweets annotated for sentiment. Load the data then do the following:\n",
    "\n",
    "- split the dataset into a train/test split.\n",
    "- create a bag of words feature representation for the tweets using the CountVectorizer\n",
    "- Use grid-search (CV) on the train split to find the best C parameters for a LinearSVC classifier. Only test 2 C values to reduce overhead (0.1 and 1.). Also, use a 2-fold CV, i.e., cv=2.\n",
    "- report (print) the accuracy of the final classifier on the test data and train data\n",
    "- How many features were created with the bag of words representation?\n",
    "\n",
    "file path: ../data/sentiment-twitter-data.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264183816548130816\t15140428\tpositive\tGas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\n",
      "264249301910310912\t18516728\tnegative\tIranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)\n",
      "264105751826538497\t147088367\tpositive\twith J Davlar 11th. Main rivals are team Poland. Hopefully we an make it a successful end to a tough week of training tomorrow.\n",
      "264094586689953794\t332474633\tnegative\tTalking about ACT's &amp;&amp; SAT's, deciding where I want to go to college, applying to colleges and everything about college stresses me out.\n",
      "254941790757601280\t557103111\tnegative\tThey may have a SuperBowl in Dallas, but Dallas ain't winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll\n",
      "264169034155696130\t382403760\tneutral\tIm bringing the monster load of candy tomorrow, I just hope it doesn't get all squiched\n",
      "263192091700654080\t344222239\tobjective-OR-neutral\tApple software, retail chiefs out in overhaul: SAN FRANCISCO Apple Inc CEO Tim Cook on Monday replaced the heads... http://t.co/X49ZEOsG\n",
      "263398998675693568\t812957996\tpositive\t@oluoch @victor_otti @kunjand I just watched it! Sridevi's comeback.... U remember her from the 90s?? Sun mornings on NTA ;)\n",
      "260200142420992000\t332530284\tobjective\t#Livewire Nadal confirmed for Mexican Open in February: Rafael Nadal is set to play at the Me... http://t.co/zgUXpcnC #LiveWireAthletics\n",
      "264087629237202944\t61903760\tpositive\t@MsSheLahY I didnt want to just pop up... but yep we have chapel hill next wednesday you should come.. and shes great ill tell her you asked\n"
     ]
    }
   ],
   "source": [
    "# This is a tab seperated file, so with csv reader use delimiter=\"\\t\"\n",
    "with open('../data/sentiment-twitter-data.tsv') as in_file:\n",
    "    count = 0\n",
    "    for row in in_file:\n",
    "        print(row.strip())\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6401, 18118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={'C': [0.1, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "X_txt = []\n",
    "y = []\n",
    "with open('../data/sentiment-twitter-data.tsv') as in_file:\n",
    "    iCSV = csv.reader(in_file, delimiter='\\t')\n",
    "    for row in iCSV:\n",
    "        X_txt.append(row[3])\n",
    "        y.append(row[2])\n",
    "        \n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(X_txt, y,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "vec.fit(X_train_txt)\n",
    "\n",
    "X_train = vec.transform(X_train_txt)\n",
    "X_test = vec.transform(X_test_txt)\n",
    "\n",
    "#X_tr\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "params = {'C': [0.1, 1.]}\n",
    "\n",
    "clf = GridSearchCV(LinearSVC(random_state=42), params, cv=10, scoring='f1_micro')\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.452585533510389"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46595877576514677\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
